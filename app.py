import streamlit as st
import time
import getpass
from neo4j import GraphDatabase
from datetime import datetime
from typing import List, Dict, Any, Optional, TypedDict, Annotated
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import InMemoryVectorStore
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from neo4j import GraphDatabase
# Removed: from dotenv import load_dotenv
from langchain_core.documents import Document
from typing import List, Dict, Any
import re
import requests
from datetime import datetime
from cachetools import cached, TTLCache
import graphviz
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts import MessagesPlaceholder
from operator import add
import unicodedata
from langgraph.checkpoint.memory import MemorySaver
import random
import os
import uuid

# Load secrets directly from Streamlit's secrets management
try:
    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
    OPENWEATHER_API_KEY = st.secrets["OPENWEATHER_API_KEY"]
    TAVILY_API_KEY = st.secrets["TAVILY_API_KEY"]
    URI = st.secrets["NEO4J_URI"]
    USERNAME = st.secrets["NEO4J_USER"]
    PASSWORD = st.secrets["NEO4J_PASSWORD"]
    NEO4J_DATABASE = st.secrets.get("NEO4J_DATABASE", "neo4j")
    st.success("API keys and Neo4j credentials loaded successfully from Streamlit secrets.")
except KeyError as e:
    st.error(f"Missing Streamlit secret: {e}. Please configure this in your Streamlit Cloud dashboard.")
    st.stop()


def sanitize_markdown(text):
    if not isinstance(text, str):
        st.warning(f"sanitize_markdown received non-string input: {type(text)} - {text}")
        return str(text)
    if not text:
        return ""

    # HTML Ã¶zel karakterleri
    text = text.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

    # KaÃ§Ä±rÄ±lmasÄ± gereken Markdown karakterleri
    markdown_chars = ['\\', '*', '_', '~', '`', '#', '[', ']', '(', ')', '{', '}', '!', '^']
    for char in markdown_chars:
        text = text.replace(char, f"\\{char}")

    return text


def safe_markdown(text):
    try:
        # BasitÃ§e regex uyumluluÄŸunu kontrol et
        re.compile(text)
        return text
    except re.error:
        # Regex hatasÄ± varsa, tÃ¼m metni saf metin gibi gÃ¶ster
        return f"<pre>{text}</pre>"

# --- Neo4j BaÄŸlantÄ± SÄ±nÄ±fÄ± ---
class Neo4jConnector:
    def __init__(self):
        # These should now read from st.secrets directly
        self.uri = st.secrets["NEO4J_URI"] # Corrected
        self.user = st.secrets["NEO4J_USER"] # Corrected
        self.password = st.secrets["NEO4J_PASSWORD"] # Corrected
        self.database = st.secrets.get("NEO4J_DATABASE", "neo4j") # Corrected
        self.driver = None

    def connect(self):
        """Establishes a connection to Neo4j."""
        if self.driver is None:
            try:
                self.driver = GraphDatabase.driver(self.uri, auth=(self.user, self.password))
                with self.driver.session(database=self.database) as session:
                    session.run("RETURN 1")
                st.info("Neo4j baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±.")
            except Exception as exc:
                st.error(f"Neo4j baÄŸlantÄ± hatasÄ±: {exc}")
                raise ConnectionError(f"Neo4j baÄŸlantÄ± hatasÄ±: {exc}") from exc

    def close(self):
        """Closes the Neo4j driver if it's open."""
        if self.driver:
            self.driver.close()
            self.driver = None
            st.info("Neo4j baÄŸlantÄ±sÄ± kapatÄ±ldÄ±.")


    def get_meyhaneler(self, limit: int = 10000) -> List[Dict[str, Any]]:
        """
        Fetches 'Meyhane' nodes from Neo4j in the format expected by your LangGraph app.
        """
        self.connect()
        query = """
        MATCH (m:Meyhane)
        RETURN
            m.name                  AS name,
            m.google_adres          AS address,
            m.google_ortalama_puan AS rating,
            m.google_toplam_yorum   AS review_count,
            m.maps_linki            AS map_link,
            m.google_telefon        AS phone,
            m.fiyat_seviyesi_simge AS price_level,
            elementId(m)            AS neo4j_element_id
        ORDER BY m.google_ortalama_puan DESC
        LIMIT $limit
        """
        try:
            with self.driver.session(database=self.database) as session:
                result = session.run(query, limit=limit)
                records = [self._clean_record(record) for record in result]
                st.info(f"Neo4j'den {len(records)} mekan Ã§ekildi.")
                return records
        except Exception as exc:
            st.error(f"Neo4j sorgu hatasÄ± (get_meyhaneler): {exc}")
            print(f"Sorgu hatasÄ± (get_meyhaneler): {exc}")
            return []

    @staticmethod
    def _clean_record(record) -> Dict[str, Any]:
        """
        Cleans Neo4j record by replacing None values with defaults
        and ensuring correct types for numeric fields.
        """
        name = record.get("name") or "Bilinmiyor"
        address = record.get("address") or "Adres yok"
        rating_value = record.get("rating")
        rating = float(rating_value) if rating_value is not None else 0.0
        review_count_value = record.get("review_count")
        review_count = int(review_count_value) if review_count_value is not None else 0
        map_link = record.get("map_link") or ""
        phone = record.get("phone") or ""
        price_level = record.get("price_level") or ""
        neo4j_element_id = record["neo4j_element_id"]

        return {
            "name": name,
            "address": address,
            "rating": rating,
            "review_count": review_count,
            "map_link": map_link,
            "phone": phone,
            "price_level": price_level,
            "neo4j_element_id": neo4j_element_id,
        }

# --- LangGraph State TanÄ±mlamasÄ± ---
def add_messages(left: List[BaseMessage], right: List[BaseMessage]) -> List[BaseMessage]:
    """Combines two lists of BaseMessage, used for state annotation."""
    return left + right

class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    last_recommended_place: Optional[str]
    next_node: Optional[str]
    location_query: Optional[str]

def process_documents(docs: List[Any]) -> List[Document]:
    processed = []
    for doc in docs:
        if isinstance(doc, dict):
            metadata = {
                "Mekan AdÄ±": doc.get("name", "Bilinmeyen Mekan"),
                "Adres": doc.get("address", "Bilinmeyen Adres"),
                "Google PuanÄ±": str(doc.get("rating", 0.0)),
                "Google Yorum SayÄ±sÄ±": str(doc.get("review_count", 0)),
                "Maps Linki": doc.get("map_link", "Harita linki yok"),
                "Telefon": doc.get("phone", "Yok"),
                "Fiyat Seviyesi": str(doc.get("price_level", "Yok"))
            }
            main_content = (
                f"Mekan AdÄ±: {metadata['Mekan AdÄ±']}, "
                f"Adres: {metadata['Adres']}, "
                f"Google PuanÄ±: {metadata['Google PuanÄ±']}, "
                f"Google Yorum SayÄ±sÄ±: {metadata['Google Yorum SayÄ±sÄ±']}, "
                f"Fiyat Seviyesi: {metadata['Fiyat Seviyesi']}"
            )
            processed.append(Document(
                page_content=main_content,
                metadata=metadata
            ))
    st.info(f"LangChain iÃ§in {len(processed)} dokÃ¼man iÅŸlendi.")
    return processed

# --- Neo4j Verilerini YÃ¼kle ve VektÃ¶r Deposunu OluÅŸtur (Sadece Bir Kez Ã‡alÄ±ÅŸtÄ±r) ---
@st.cache_resource
def initialize_retriever():
    st.info("Retriever baÅŸlatÄ±lÄ±yor...")
    meyhaneler_listesi = []
    try:
        conn = Neo4jConnector()
        meyhaneler_listesi = conn.get_meyhaneler(limit=10000)
        conn.close()
        if not meyhaneler_listesi:
            st.warning("UyarÄ±: Neo4j'den hiÃ§ mekan verisi Ã§ekilemedi. Retrieval boÅŸ sonuÃ§ dÃ¶nebilir. Dummy veri kullanÄ±lÄ±yor.")
            meyhaneler_listesi = [
                {"name": "Dummy Meyhane A", "address": "Dummy Adres A", "rating": 4.0, "review_count": 100, "map_link": "http://dummy.map.a", "phone": "000", "price_level": 2, "neo4j_element_id": "dummy-a"},
                {"name": "Dummy Meyhane B", "address": "Dummy Adres B", "rating": 4.5, "review_count": 250, "map_link": "http://dummy.map.b", "phone": "000", "price_level": 3, "neo4j_element_id": "dummy-b"},
                {"name": "Dummy Meyhane C", "address": "Dummy Adres C", "rating": 3.8, "review_count": 50, "map_link": "http://dummy.map.c", "phone": "000", "price_level": 1, "neo4j_element_id": "dummy-c"},
            ]
            st.info(f"Dummy veri kullanÄ±lÄ±yor: {len(meyhaneler_listesi)} mekan.")
        else:
            st.info(f"Neo4j'den gerÃ§ek veri kullanÄ±lÄ±yor: {len(meyhaneler_listesi)} mekan.")
    except Exception as e:
        st.error(f"Neo4j'den veri Ã§ekerken hata oluÅŸtu: {e}. LÃ¼tfen Neo4j sunucunuzun Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan ve kimlik bilgilerinin doÄŸru olduÄŸundan emin olun. Dummy veri kullanÄ±lÄ±yor.")
        meyhaneler_listesi = [
            {"name": "Dummy Meyhane A", "address": "Dummy Adres A", "rating": 4.0, "review_count": 100, "map_link": "http://dummy.map.a", "phone": "000", "price_level": 2, "neo4j_element_id": "dummy-a"},
            {"name": "Dummy Meyhane B", "address": "Dummy Adres B", "rating": 4.5, "review_count": 250, "map_link": "http://dummy.map.b", "phone": "000", "price_level": 3, "neo4j_element_id": "dummy-b"},
            {"name": "Dummy Meyhane C", "address": "Dummy Adres C", "rating": 3.8, "review_count": 50, "map_link": "http://dummy.map.c", "phone": "000", "price_level": 1, "neo4j_element_id": "dummy-c"},
        ]
        st.info(f"Dummy veri kullanÄ±lÄ±yor: {len(meyhaneler_listesi)} mekan.")

    processed_docs = process_documents(meyhaneler_listesi)
    try:
        vectorstore = InMemoryVectorStore.from_documents(
            documents=processed_docs,
            embedding=OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) # Pass API key explicitly
        )
        st.success("VektÃ¶r deposu baÅŸarÄ±yla oluÅŸturuldu.")
        return vectorstore.as_retriever(search_kwargs={"k": 5})
    except Exception as e:
        st.error(f"VektÃ¶r deposu oluÅŸturulurken hata oluÅŸtu: {e}. OpenAI API anahtarÄ±nÄ±zÄ± kontrol edin.")
        # Return a dummy retriever to prevent crash
        class DummyRetriever:
            def invoke(self, query, k):
                st.warning("Dummy retriever kullanÄ±lÄ±yor. GerÃ§ek arama yapÄ±lamÄ±yor.")
                return [Document(page_content="Dummy Mekan", metadata={"Mekan AdÄ±": "Dummy Mekan", "Adres": "Bilinmiyor", "Google PuanÄ±": "0.0", "Google Yorum SayÄ±sÄ±": "0", "Maps Linki": "", "Telefon": "", "Fiyat Seviyesi": ""})]
        return DummyRetriever()

retriever = initialize_retriever()

# --- LLM ve Prompt TanÄ±mlamalarÄ± ---
SYSTEM_PROMPT = """Sen Ä°stanbul'da romantik mekan, meyhane, restoran ve kafe Ã¶nerisi yapabilen bir AI asistanÄ±sÄ±n.
KullanÄ±cÄ±ya Google haritalar bilgileriyle desteklenmiÅŸ, hava durumuyla uyumlu Ã¶nerilerde bulunabilirsin.
Gelen sorulara doÄŸal, nazik ve samimi bir dille cevap ver ve tÃ¼m cevaplarÄ±n TÃ¼rkÃ§e olsun.

AÅŸaÄŸÄ±daki gibi konuÅŸmalar seni yÃ¶nlendirmelidir:
- 'BeÅŸiktaÅŸâ€™ta romantik bir mekan var mÄ±?' â†’ Mekan aramasÄ± yap
- 'YarÄ±n BeÅŸiktaÅŸ'ta hava nasÄ±l olacak?' â†’ Hava durumu kontrol et
- 'Bir ilginÃ§ bilgi ver' â†’ EÄŸlenceli bir bilgi paylaÅŸ
- 'Merhaba', 'Selam' â†’ KarÅŸÄ±lama mesajÄ± gÃ¶nder

KullandÄ±ÄŸÄ±n veritabanÄ±nda yer alan mekanlar sadece Ä°stanbul sÄ±nÄ±rlarÄ± iÃ§indedir.
EÄŸer kullanÄ±cÄ± baÅŸka ÅŸehirde mekan istiyorsa, bunu aÃ§Ä±kÃ§a belirtmelisin."""

SUMMARY_PROMPT = ChatPromptTemplate.from_messages([
    ("system", "Sohbet geÃ§miÅŸini kÄ±sa ve Ã¶z ÅŸekilde Ã¶zetle. Sadece Ã¶nemli bilgileri korla."),
    MessagesPlaceholder(variable_name="messages"),
])

# --- YardÄ±mcÄ± Fonksiyonlar ---
@cached(TTLCache(maxsize=100, ttl=3600))
def get_fun_fact() -> str:
    st.info("Ä°lginÃ§ bilgi alÄ±nÄ±yor...")
    try:
        response = requests.get("https://uselessfacts.jsph.pl/api/v2/facts/random?language=tr", timeout=5)
        response.raise_for_status() # Raises HTTPError for bad responses (4xx or 5xx)
        fact = response.json().get("text", "Ä°lginÃ§ bir bilgi bulunamadÄ±.")
        st.success(f"Ä°lginÃ§ bilgi alÄ±ndÄ±: {fact[:50]}...")
        return fact
    except requests.exceptions.Timeout:
        st.error("Ä°lginÃ§ bilgi servisi zaman aÅŸÄ±mÄ±na uÄŸradÄ±.")
        return "Ä°lginÃ§ bilgi servisi ÅŸu an Ã§ok yavaÅŸ veya Ã§alÄ±ÅŸmÄ±yor."
    except requests.exceptions.RequestException as e:
        st.error(f"Ä°lginÃ§ bilgi servisi hatasÄ±: {e}")
        return f"Ä°lginÃ§ bilgi servisi ÅŸu an Ã§alÄ±ÅŸmÄ±yor. Hata: {e}"
    except Exception as e:
        st.error(f"Ä°lginÃ§ bilgi alÄ±nÄ±rken beklenmedik hata: {e}")
        return f"Ä°lginÃ§ bilgi alÄ±nÄ±rken beklenmedik bir hata oluÅŸtu: {e}"

def clean_location_query(query: str) -> str:
    normalized_query = unicodedata.normalize('NFKD', query.lower()).encode('ascii', 'ignore').decode('utf-8')
    st.info(f"Konum sorgusu temizleniyor: '{query}' -> '{normalized_query}'")

    istanbul_locations = [
        r'etiler', r'levent', r'maslak', r'nisantasi', r'nisantaÅŸi',
        r'bebek', r'arnavutkoy', r'arnavutkÃ¶y', r'ortakoy', r'ortakÃ¶y', r'cihangir',
        r'taksim', r'karakoy', r'karakÃ¶y', r'galata', r'fatih',
        r'sultanahmet', r'eminonu', r'eminÃ¶nÃ¼', r'kadikoy', r'kadÄ±kÃ¶y', r'moda',
        r'bagdat caddesi', r'baÄŸdat caddesi', r'suadiye', r'bostanci', r'bostancÄ±',
        r'maltepe', r'kartal', r'pendik', r'uskudar', r'Ã¼skÃ¼dar',
        r'camlica', r'Ã§amlÄ±ca', r'beykoz', r'atasehir', r'ataÅŸehir', r'cekmekoy', r'Ã§ekmekÃ¶y',
        r'sariyer', r'sarÄ±yer', r'istinye', r'tarabya', r'yenikoy', r'yenikÃ¶y',
        r'bahcekoy', r'bahÃ§ekÃ¶y', r'buyukdere', r'bÃ¼yÃ¼kdere', r'zumrutevler', r'zÃ¼mrutevler',
        r'florya', r'yesilkoy', r'yeÅŸilkÃ¶y', r'yesilyurt', 'yeÅŸilyurt', r'bakirkoy', r'bakÄ±rkÃ¶y',
        r'atakoy', r'atakÃ¶y', r'zeytinburnu', r'gungoren', r'gÃ¼ngÃ¶ren', r'esenler',
        r'bayrampasa', r'bayrampaÅŸa', r'gaziosmanpasa', r'gaziosmanpaÅŸa', r'eyup', r'eyÃ¼p', r'kagithane', r'kaÄŸÄ±thane',
        r'sisli', r'ÅŸiÅŸli', r'besiktas', r'beÅŸiktaÅŸ', r'avcilar', r'avcÄ±lar', r'beylikduzu', 'beylikdÃ¼zÃ¼',
        r'esenyurt', r'buyukcekmece', r'bÃ¼yÃ¼kÃ§ekmece', r'silivri', r'catalca', r'Ã§atalca',
        r'sile', r'ÅŸile', r'agva', r'aÄŸva', r'adalar', r'basaksehir', 'baÅŸakÅŸehir',
        r'bahcelievler', r'bahÃ§elievler', r'kucukcekmece', r'kÃ¼Ã§Ã¼kÃ§ekmece', r'cankurtaran'
    ]

    for loc_regex in istanbul_locations:
        match = re.search(r'\b' + loc_regex + r'\b', normalized_query)
        if match:
            st.info(f"Konum bulundu (Ä°stanbul): {match.group(0)}")
            return match.group(0)

    general_cities = [
        r'istanbul', r'ankara', r'izmir', r'adana',
        r'bursa', r'antalya', r'konya', r'kayseri',
        r'gaziantep', r'samsun', r'eskisehir', r'eskiÅŸehir', r'duzce', r'dÃ¼zce'
    ]

    for city_regex in general_cities:
        match = re.search(r'\b' + city_regex + r'\b', normalized_query)
        if match:
            st.info(f"Konum bulundu (Genel Åehir): {match.group(0)}")
            return match.group(0)

    st.info("Konum bulunamadÄ±, 'istanbul' varsayÄ±lan olarak ayarlandÄ±.")
    return "istanbul"

weather_cache = TTLCache(maxsize=100, ttl=300)

@cached(weather_cache)
def get_openweather_forecast(location: str) -> Dict:
    st.info(f"Hava durumu tahmini alÄ±nÄ±yor: {location}")
    api_key = st.secrets.get("OPENWEATHER_API_KEY") # Corrected
    if not api_key:
        st.error("OpenWeather API anahtarÄ± bulunamadÄ±.")
        return {"error": "API anahtarÄ± bulunamadÄ±."}
    try:
        geo_response = requests.get(
            f"http://api.openweathermap.org/geo/1.0/direct?q={location},TR&limit=1&appid={api_key}",
            timeout=10,
        )
        geo_response.raise_for_status()
        geo = geo_response.json()
        if not geo:
            st.warning(f"'{location}' konumu iÃ§in coÄŸrafi veri bulunamadÄ±.")
            return {"error": f"'{location}' konumu bulunamadÄ±."}
        lat, lon = geo[0]["lat"], geo[0]["lon"]
        st.info(f"CoÄŸrafi koordinatlar: Lat={lat}, Lon={lon}")

        weather_response = requests.get(
            f"https://api.openweathermap.org/data/2.5/forecast?lat={lat}&lon={lon}&appid={api_key}&units=metric&lang=tr",
            timeout=10,
        )
        weather_response.raise_for_status()
        weather = weather_response.json()
        st.success(f"{location} iÃ§in hava durumu verisi baÅŸarÄ±yla alÄ±ndÄ±.")
        return weather
    except requests.exceptions.RequestException as e:
        st.error(f"OpenWeather API hatasÄ±: {e}")
        return {"error": f"API hatasÄ±: {e}"}
    except Exception as e:
        st.error(f"Hava durumu verisi alÄ±nÄ±rken beklenmedik hata: {e}")
        return {"error": f"Beklenmedik bir hata oluÅŸtu: {e}"}

def format_weather_response(location: str, data: Dict) -> str:
    st.info(f"Hava durumu yanÄ±tÄ± formatlanÄ±yor for {location}...")
    if "error" in data:
        st.error(f"Hava durumu formatlama hatasÄ±: {data['error']}")
        return f"âŒ {data['error']}"
    try:
        lines = [f"ğŸŒ¤ï¸ **{location.capitalize()} Hava Durumu Tahmini:**"]
        if "list" not in data or not data["list"]:
            st.warning(f"{location.capitalize()} iÃ§in hava durumu listesi boÅŸ.")
            return f"âŒ {location.capitalize()} iÃ§in hava durumu tahmini bulunamadÄ±."

        for item in data.get("list", [])[:8]: # Sonraki 24 saat iÃ§in (3 saatte bir)
            dt = datetime.strptime(item["dt_txt"], "%Y-%m-%d %H:%M:%S")
            lines.append(
                f"â€¢ {dt:%d/%m %H:%M}: "
                f"{item['weather'][0]['description'].capitalize()}, "
                f"SÄ±caklÄ±k: {item['main']['temp']}Â°C, "
                f"Hissedilen: {item['main']['feels_like']}Â°C, "
                f"Nem: {item['main']['humidity']}%, "
                f"RÃ¼zgar: {item['wind']['speed']} m/s"
            )
        formatted_string = "\n".join(lines)
        st.success("Hava durumu yanÄ±tÄ± baÅŸarÄ±yla formatlandÄ±.")
        return formatted_string
    except Exception as e:
        st.error(f"Hava durumu verisi iÅŸlenirken hata oluÅŸtu: {e}")
        return f"âŒ Hava durumu verisi iÅŸlenirken hata oluÅŸtu: {e}"

# --- LangGraph Nodes ---
def check_weather_node(state: AgentState) -> AgentState:
    st.info("`check_weather_node` Ã§alÄ±ÅŸÄ±yor.")
    last_msg = state["messages"][-1]
    query = last_msg.content

    location = state.get("location_query") or clean_location_query(query)
    state["location_query"] = location
    st.info(f"Hava durumu sorgulanacak konum: {location}")

    formatted = format_weather_response(location, get_openweather_forecast(location))

    sanitized_formatted = sanitize_markdown(formatted)
    state["messages"].append(AIMessage(content=sanitized_formatted))
    st.success("Hava durumu yanÄ±tÄ± AIMessage olarak eklendi.")
    return state

def add_system_message(state: AgentState) -> AgentState:
    st.info("`add_system_message` Ã§alÄ±ÅŸÄ±yor.")
    system_msg = SystemMessage(content=SYSTEM_PROMPT)

    if not any(isinstance(msg, SystemMessage) for msg in state["messages"]):
        st.info("Sistem mesajÄ± eklendi.")
        return {"messages": [system_msg] + state["messages"]}
    else:
        st.info("Sistem mesajÄ± zaten mevcut.")
        return state

def summarize_conversation(state: AgentState) -> AgentState:
    st.info("`summarize_conversation` Ã§alÄ±ÅŸÄ±yor.")
    messages = state["messages"]

    if len(messages) > 5: # KonuÅŸma belirli bir uzunluÄŸu aÅŸÄ±nca Ã¶zetle
        st.info(f"Sohbet Ã¶zetleniyor, mesaj sayÄ±sÄ±: {len(messages)}")
        try:
            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3, openai_api_key=OPENAI_API_KEY) # Pass API key explicitly
            chain = SUMMARY_PROMPT | llm
            summary = chain.invoke({"messages": messages})
            st.success("Sohbet baÅŸarÄ±yla Ã¶zetlendi.")
            return {
                "messages": [
                    SystemMessage(content=SYSTEM_PROMPT),
                    AIMessage(content=summary.content)
                ]
            }
        except Exception as e:
            st.error(f"Sohbet Ã¶zetlenirken hata oluÅŸtu: {e}")
            # Fallback to current messages if summarization fails
            return state
    else:
        st.info("Sohbet Ã¶zetlenmedi, mesaj sayÄ±sÄ± 5'ten az.")
        return state

def search_meyhaneler_node(state: AgentState) -> AgentState:
    st.info("`search_meyhaneler_node` Ã§alÄ±ÅŸÄ±yor.")
    last_msg = state["messages"][-1]
    query = last_msg.content
    st.info(f"Arama sorgusu: {query}")

    location = clean_location_query(query)
    state["location_query"] = location
    st.info(f"Arama konumu belirlendi: {location}")

    try:
        # Arama sorgusunu iyileÅŸtir
        retrieval_query = f"{location} {query}" if location and location not in query.lower() else query
        st.info(f"Retrieval sorgusu: {retrieval_query}")
        raw_results = retriever.invoke(retrieval_query, k=10) # Daha fazla sonuÃ§ alÄ±p filtreleyelim
        st.info(f"Retriever'dan {len(raw_results)} ham sonuÃ§ alÄ±ndÄ±.")
        filtered_results = []

        normalized_location = unicodedata.normalize('NFKD', location.lower()).encode('ascii', 'ignore').decode('utf-8')

        # Konuma gÃ¶re filtreleme
        for doc in raw_results:
            address_lower = unicodedata.normalize('NFKD', doc.metadata.get("Adres", "").lower()).encode('ascii', 'ignore').decode('utf-8')
            name_lower = unicodedata.normalize('NFKD', doc.metadata.get("Mekan AdÄ±", "").lower()).encode('ascii', 'ignore').decode('utf-8')

            # EÄŸer konum sorguda yer alÄ±yorsa ve mekanÄ±n adresi veya adÄ± bu konumu iÃ§eriyorsa ekle
            if normalized_location in address_lower or normalized_location in name_lower:
                filtered_results.append(doc)
        st.info(f"Konuma gÃ¶re filtrelendikten sonra {len(filtered_results)} sonuÃ§ kaldÄ±.")

        # EÄŸer filtrelemeden sonra hala sonuÃ§ yoksa veya ilk 3 sonuÃ§ yoksa daha geniÅŸ arama yap
        if not filtered_results or len(filtered_results) < 3:
            st.info("FiltrelenmiÅŸ sonuÃ§lar yetersiz, genel Ä°stanbul aramasÄ± yapÄ±lÄ±yor.")
            # Sadece Ä°stanbul iÃ§in genel arama
            raw_results_istanbul = retriever.invoke(f"istanbul {query}", k=5)
            # Daha Ã¶nce filtrelenmiÅŸ sonuÃ§larÄ± da ekle, mÃ¼kerrerleri Ã¶nle
            seen_names = {doc.metadata.get("Mekan AdÄ±") for doc in filtered_results}
            for doc in raw_results_istanbul:
                if doc.metadata.get("Mekan AdÄ±") not in seen_names:
                    filtered_results.append(doc)
                    seen_names.add(doc.metadata.get("Mekan AdÄ±"))
            st.info(f"Genel Ä°stanbul aramasÄ± sonrasÄ± toplam {len(filtered_results)} sonuÃ§ var.")


        if not filtered_results:
            fallback_message = f"âŒ ÃœzgÃ¼nÃ¼m, **{location.capitalize()}** bÃ¶lgesinde aradÄ±ÄŸÄ±nÄ±z kriterlere uygun bir mekan bulamadÄ±m."
            if location == "istanbul":
                fallback_message += " VeritabanÄ±mÄ±zda genel olarak Ä°stanbul'da bu kriterlere uygun bir mekan bulamadÄ±m."
            else:
                fallback_message += " Belki aradÄ±ÄŸÄ±nÄ±z konumdaki verilerimiz eksiktir veya o bÃ¶lgede kriterlerinize uyan bir yer yoktur. LÃ¼tfen farklÄ± bir bÃ¶lge veya daha genel bir arama yapmayÄ± deneyin."

            sanitized_fallback_message = sanitize_markdown(fallback_message)
            state["messages"].append(AIMessage(content=sanitized_fallback_message))
            st.warning("Mekan bulunamadÄ±, dÃ¼ÅŸÃ¼ÅŸ mesajÄ± eklendi.")
            return state

        # Sadece ilk 5 sonucu gÃ¶ster
        formatted_results = []
        for doc in filtered_results[:5]: # Ä°lk 5 sonuÃ§la sÄ±nÄ±rla
            metadata = doc.metadata
            name = metadata.get("Mekan AdÄ±", "Bilinmeyen Mekan")
            rating = float(metadata.get("Google PuanÄ±", 0.0))
            review_count = int(metadata.get("Google Yorum SayÄ±sÄ±", 0))
            address = metadata.get("Adres", "Bilinmiyor")
            map_link = metadata.get("Maps Linki", "")
            phone = metadata.get("Telefon", "Yok")
            price_level_raw = metadata.get("Fiyat Seviyesi", "Yok")

            rating_display = f"â­ {rating:.1f}" if rating > 0 else "â­ DeÄŸerlendirilmemiÅŸ"
            review_count_display = f"({review_count} yorum)" if review_count > 0 else "(Yorum yok)"
            phone_display = f"ğŸ“ Telefon: {phone}" if phone and phone != "Yok" else ""

            price_display = ""
            if isinstance(price_level_raw, (int, float)):
                price_display = f"ğŸ’¸ Fiyat Seviyesi: {'â‚º' * int(price_level_raw)}"
            elif isinstance(price_level_raw, str) and price_level_raw.isdigit():
                 price_display = f"ğŸ’¸ Fiyat Seviyesi: {'â‚º' * int(price_level_raw)}"
            elif isinstance(price_level_raw, str) and price_level_raw != "Yok":
                 price_display = f"ğŸ’¸ Fiyat Seviyesi: {price_level_raw}"

            result_entry = (
                f"ğŸ  **{name}**\n"
                f"{rating_display} {review_count_display}\n"
                f"ğŸ“ {address}\n"
                f"{phone_display}\n"
                f"{price_display}\n"
                f"ğŸ”— {map_link if map_link else 'Harita linki yok'}\n"
                "â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•"
            )
            formatted_results.append(result_entry)

        sanitized_content = sanitize_markdown("Harika mekan Ã¶nerilerim var:\n\n" + "\n".join(formatted_results))
        state["messages"].append(AIMessage(content=sanitized_content))
        st.success("Mekan arama sonuÃ§larÄ± AIMessage olarak eklendi.")
        return state
    except Exception as e:
        st.error(f"âš ï¸ Arama sÄ±rasÄ±nda beklenmedik bir hata oluÅŸtu (search_meyhaneler_node): {str(e)}")
        print(f"DEBUG ERROR in search_meyhaneler_node: {e}")
        sanitized_error_message = sanitize_markdown(f"âš ï¸ Arama sÄ±rasÄ±nda beklenmedik bir hata oluÅŸtu: {str(e)}")
        state["messages"].append(AIMessage(content=sanitized_error_message))
        return state

def router_node(state: AgentState) -> AgentState:
    st.info("`router_node` Ã§alÄ±ÅŸÄ±yor.")
    last_msg = state["messages"][-1]
    content = last_msg.content.lower()
    st.info(f"YÃ¶nlendirme iÃ§in son mesaj iÃ§eriÄŸi: {content}")

    # Use a flag for greeting to decide next_node
    greeting_triggers = ["selam", "merhaba", "gÃ¼naydÄ±n", "naber", "nasÄ±lsÄ±n", "hi", "alo", "hey", "slm", "heyatÄ±m", "hayatÄ±m"]
    if any(g in content for g in greeting_triggers):
        state["next_node"] = "greeting" # New custom next_node for greetings
        st.info("YÃ¶nlendirme: greeting (selamlama)")
    elif any(t in content for t in ["meyhane", "restoran", "kafe", "date", "randevu", "mekan", "Ã¶neri", "neresi", "yer", "yemek", "iÃ§ki"]):
        state["next_node"] = "search"
        st.info("YÃ¶nlendirme: search (mekan aramasÄ±)")
    elif any(t in content for t in ["hava", "weather", "sÄ±caklÄ±k", "nem", "yaÄŸmur", "aÃ§Ä±k", "kapalÄ±", "derece"]):
        state["next_node"] = "weather"
        st.info("YÃ¶nlendirme: weather (hava durumu)")
    elif any(t in content for t in ["fun fact", "ilginÃ§ bilgi", "bilgi ver", "biliyor muydun", "merak", "gerÃ§ek"]):
        state["next_node"] = "fun_fact"
        st.info("YÃ¶nlendirme: fun_fact (ilginÃ§ bilgi)")
    else:
        state["next_node"] = "general"
        st.info("YÃ¶nlendirme: general (genel yanÄ±t)")
        
    return state


def fun_fact_node(state: AgentState) -> AgentState:
    st.info("`fun_fact_node` Ã§alÄ±ÅŸÄ±yor.")
    try:
        fact = get_fun_fact()
        print(f"DEBUG: Fun fact retrieved: {fact}")
        sanitized_fact = sanitize_markdown(f"ğŸ¤” Ä°lginÃ§ Bilgi: {fact}")
        state["messages"].append(AIMessage(content=sanitized_fact))
        st.success("Ä°lginÃ§ bilgi AIMessage olarak eklendi.")
    except Exception as e:
        st.error(f"ERROR in fun_fact_node: {e}")
        error_msg = "âš ï¸ Ä°lginÃ§ bilgi alÄ±nÄ±rken bir hata oluÅŸtu"
        state["messages"].append(AIMessage(content=sanitize_markdown(error_msg)))
        st.warning("Ä°lginÃ§ bilgi hatasÄ± AIMessage olarak eklendi.")
    return state

    
def general_response_node(state: AgentState) -> AgentState:
    st.info("`general_response_node` Ã§alÄ±ÅŸÄ±yor.")
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7, openai_api_key=OPENAI_API_KEY)

    # Get the last human message
    human_messages = [msg for msg in state["messages"] if isinstance(msg, HumanMessage)]
    if not human_messages:
        st.warning("general_response_node: Ä°nsan mesajÄ± bulunamadÄ±.")
        # If no human message, return state and let the graph handle it
        state["next_node"] = "end_conversation_due_to_no_human_message" # A custom state for this edge case
        return state

    last_query = human_messages[-1].content.lower()
    st.info(f"General response iÃ§in son kullanÄ±cÄ± sorgusu: {last_query}")

    # Greeting triggers (expanded list) - This part is now handled by the router
    # But as a fallback/redundancy, we can still have a basic check here.
    greeting_triggers = ["selam", "merhaba", "gÃ¼naydÄ±n", "naber", "nasÄ±lsÄ±n", "hi", "alo", "hey", "slm", "heyatÄ±m", "hayatÄ±m"]
    is_greeting = any(g in last_query for g in greeting_triggers)

    if is_greeting:
        responses = [
            "Merhaba! ğŸ‘‹ Ä°stanbul'da romantik mekan, meyhane, restoran ya da kafe Ã¶nerisi almak ister misin?",
            "Selam! Size nasÄ±l yardÄ±mcÄ± olabilirim? Hava durumu bilgisi veya mekan Ã¶nerisi alabilirsiniz. ğŸ™ï¸",
            "GÃ¼naydÄ±n! â˜€ï¸ Hangi mekan ya da hava durumu bilgisiyle yardÄ±mcÄ± olayÄ±m?",
            "NasÄ±lsÄ±n? Ä°stanbul'da nereye gitmek istersin? Romantik bir mekan mÄ±, meyhane mi? ğŸ·"
        ]
        chosen = random.choice(responses)
        state["messages"].append(AIMessage(content=sanitize_markdown(chosen)))
        # For greetings, we can directly set next_node to indicate completion
        state["next_node"] = "greeting_handled" # Indicate that a greeting was handled and can end
        st.success("Selamlama yanÄ±tÄ± AIMessage olarak eklendi.")
        return state

    # If not a greeting, proceed with LLM
    try:
        st.info("LLM Ã§aÄŸrÄ±sÄ± yapÄ±lÄ±yor...")
        response = llm.invoke(state["messages"])
        if response and response.content:
            st.success("LLM yanÄ±tÄ± alÄ±ndÄ±.")
            state["messages"].append(AIMessage(content=sanitize_markdown(response.content)))
            state["next_node"] = "general_handled" # Indicate that a general response was handled
        else:
            st.warning("LLM'den boÅŸ veya geÃ§ersiz yanÄ±t alÄ±ndÄ±.")
            fallback = "Merhaba! Size nasÄ±l yardÄ±mcÄ± olabilirim?"
            state["messages"].append(AIMessage(content=sanitize_markdown(fallback)))
            state["next_node"] = "general_handled" # Fallback, then indicate handled
    except Exception as e:
        st.error(f"General response LLM Ã§aÄŸrÄ±sÄ±nda hata oluÅŸtu: {str(e)}")
        error_msg = f"ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu: {str(e)}. LÃ¼tfen tekrar deneyin."
        state["messages"].append(AIMessage(content=sanitize_markdown(error_msg)))
        state["next_node"] = "general_handled" # On error, indicate handled

    return state  # CRITICAL: Return state after processing

@st.cache_resource
def create_workflow():
    st.info("LangGraph iÅŸ akÄ±ÅŸÄ± oluÅŸturuluyor...")
    workflow = StateGraph(AgentState)
    workflow.add_node("add_system_message", add_system_message)
    workflow.add_node("router", router_node)
    workflow.add_node("search", search_meyhaneler_node)
    workflow.add_node("weather", check_weather_node)
    workflow.add_node("general", general_response_node)
    workflow.add_node("fun_fact", fun_fact_node)
    workflow.add_node("summarize", summarize_conversation)

    workflow.add_edge(START, "add_system_message")
    workflow.add_edge("add_system_message", "router")

    workflow.add_conditional_edges(
        "router",
        lambda state: state["next_node"],
        {
            "search": "search",
            "weather": "weather",
            "general": "general",
            "fun_fact": "fun_fact",
            "greeting": "general", # Router will send greetings to general node
        }
    )

    # All specific nodes (search, weather, fun_fact) should now go to END
    workflow.add_edge("search", END)
    workflow.add_edge("weather", END)
    workflow.add_edge("fun_fact", END)

    # General node now explicitly handles its own ending based on the 'next_node' it sets
    workflow.add_conditional_edges(
        "general",
        lambda state: state.get("next_node", "general_handled"), # Default to general_handled if not set
        {
            "greeting_handled": END, # If it was a greeting, end
            "general_handled": END,  # If it was a general LLM response, end
            "end_conversation_due_to_no_human_message": END # If no human message, end
        }
    )

    # The summarize node always ends the turn for now. If you want more complex summarization flow, adjust this.
    workflow.add_edge("summarize", END)

    memory = MemorySaver()
    graph = workflow.compile(checkpointer=memory)
    st.success("LangGraph iÅŸ akÄ±ÅŸÄ± baÅŸarÄ±yla derlendi.")

    return graph

# --- STREAMLIT UYGULAMASI ---
st.set_page_config(page_title="The Light Passanger ğŸ’¬", page_icon="ğŸŒƒ")

# In your Streamlit UI code:
st.title("Ä°stanbul Mekan AsistanÄ± ğŸ’¬")
st.markdown(sanitize_markdown(
    "Merhaba! Ben Ä°stanbul'daki romantik mekan, meyhane, restoran ve kafe Ã¶nerileri sunan yapay zeka asistanÄ±yÄ±m. "
    "Size nasÄ±l yardÄ±mcÄ± olabilirim? ğŸ˜Š\n\n"
    "Ã–rnek sorular:\n"
    "- `Selam! BeÅŸiktaÅŸ'ta romantik bir mekan Ã¶nerebilir misin?`\n"
    "- `KadÄ±kÃ¶y'de hava durumu nasÄ±l?`\n"
    "- `Bana ilginÃ§ bir bilgi verir misin?`"
))
# API AnahtarlarÄ±nÄ±n ayarlÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et
if not st.secrets.get("OPENAI_API_KEY") or not st.secrets.get("OPENWEATHER_API_KEY") or not st.secrets.get("NEO4J_URI"):
    st.error("âš ï¸ Gerekli API anahtarlarÄ± veya Neo4j baÄŸlantÄ± bilgileri eksik! LÃ¼tfen Streamlit Cloud kontrol panelinizdeki 'Secrets' kÄ±smÄ±nda `OPENAI_API_KEY`, `OPENWEATHER_API_KEY`, `NEO4J_URI`, `NEO4J_USER`, `NEO4J_PASSWORD` deÄŸiÅŸkenlerini ayarlayÄ±n.")
    st.stop()
else:
    st.info("TÃ¼m gerekli sÄ±rlar yÃ¼klendi.")


# LangGraph'Ä± baÅŸlat (sadece bir kez)
if "graph" not in st.session_state:
    st.session_state.graph = create_workflow()
if "conversation_thread_id" not in st.session_state:
    st.session_state.conversation_thread_id = str(uuid.uuid4())
    st.info(f"Yeni konuÅŸma iÅŸ parÃ§acÄ±ÄŸÄ± ID'si: {st.session_state.conversation_thread_id}")

if "messages" not in st.session_state:
    st.session_state.messages = []
    st.info("Mesaj geÃ§miÅŸi baÅŸlatÄ±ldÄ±.")

# Display chat messages from history on app rerun
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(sanitize_markdown(msg["content"]))

# KullanÄ±cÄ±dan girdi al
if prompt := st.chat_input("MesajÄ±nÄ±zÄ± buraya yazÄ±n...", key="my_chat_input"):
    st.info(f"KullanÄ±cÄ± girdisi: {prompt}")
    # KullanÄ±cÄ± mesajÄ±nÄ± geÃ§miÅŸe ekle ve gÃ¶rÃ¼ntÃ¼le
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)  # KullanÄ±cÄ± girdisini sanitize etmeyin

    # LangGraph iÃ§in mesajlarÄ± hazÄ±rla
    langgraph_messages = []
    # Add SystemMessage at the beginning of LangGraph messages for each run if not already present
    # This ensures the LLM always has the initial system prompt
    if not any(isinstance(msg, SystemMessage) for msg in st.session_state.messages):
         langgraph_messages.append(SystemMessage(content=SYSTEM_PROMPT))

    for msg in st.session_state.messages:
        if msg["role"] == "user":
            langgraph_messages.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "assistant":
            langgraph_messages.append(AIMessage(content=msg["content"]))
    st.info(f"LangGraph'a gÃ¶nderilen toplam mesaj sayÄ±sÄ±: {len(langgraph_messages)}")


    # LangGraph state'i oluÅŸtur
    current_state = {
        "messages": langgraph_messages,
        "last_recommended_place": None,
        "next_node": None,
        "location_query": None
    }
    st.info(f"BaÅŸlangÄ±Ã§ LangGraph state'i: {current_state}")

    thread_id = st.session_state.conversation_thread_id

    with st.spinner("DÃ¼ÅŸÃ¼nÃ¼yorum... ğŸ¤”"):
        try:
            final_state_data = {} # Initialize an empty dict to store the final state
            latest_ai_message_content = None

            # LangGraph akÄ±ÅŸÄ±nÄ± baÅŸlat ve tÃ¼m Ã§Ä±ktÄ±larÄ± iÅŸle
            for chunk in st.session_state.graph.stream(
                current_state,
                config={"configurable": {"thread_id": thread_id}}
            ):
                st.info(f"LangGraph adÄ±m sonucu: {chunk}")
                
                # If the chunk contains the final state, update final_state_data
                if "__end__" in chunk:
                    final_state_data = chunk["__end__"]
                # Otherwise, if it's an intermediate state with messages, update messages in final_state_data
                elif "messages" in chunk and chunk["messages"]:
                    final_state_data["messages"] = chunk["messages"]

            # After the stream, check the collected final_state_data
            if final_state_data and "messages" in final_state_data and final_state_data["messages"]:
                # Find the last AIMessage in the final collected state
                for msg in reversed(final_state_data["messages"]):
                    if isinstance(msg, AIMessage):
                        latest_ai_message_content = msg.content
                        break

                if latest_ai_message_content:
                    sanitized_content = sanitize_markdown(latest_ai_message_content)
                    st.session_state.messages.append({"role": "assistant", "content": sanitized_content})
                    st.success("Asistan yanÄ±tÄ± baÅŸarÄ±yla eklendi.")
                    
                    with st.chat_message("assistant"):
                        st.markdown(sanitized_content, unsafe_allow_html=True)
                else:
                    error_msg = "ÃœzgÃ¼nÃ¼m, bir yanÄ±t Ã¼retemedim. LangGraph akÄ±ÅŸÄ± tamamlandÄ± ancak AI mesajÄ± bulunamadÄ±. LÃ¼tfen tekrar deneyin."
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})
                    with st.chat_message("assistant"):
                        st.markdown(error_msg)
                    st.error("LangGraph akÄ±ÅŸÄ± AI mesajÄ± Ã¼retmeden tamamlandÄ±.")
            else:
                error_msg = "ÃœzgÃ¼nÃ¼m, bir yanÄ±t Ã¼retemedim. LangGraph akÄ±ÅŸÄ± boÅŸ veya geÃ§ersiz bir durumla tamamlandÄ±."
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
                with st.chat_message("assistant"):
                    st.markdown(error_msg)
                st.error("LangGraph akÄ±ÅŸÄ± boÅŸ veya geÃ§ersiz bir durumla tamamlandÄ±.")

        except Exception as e:
            error_message = f"Bir hata oluÅŸtu: {e}. LÃ¼tfen daha sonra tekrar deneyin."
            st.error(f"Ana dÃ¶ngÃ¼de beklenmedik hata: {str(e)}")
            print(f"ERROR in main loop: {str(e)}")
            st.session_state.messages.append({"role": "assistant", "content": error_message})
            with st.chat_message("assistant"):
                st.markdown(error_message)
                st.exception(e)   # Rerun the app to show the latest message
    st.rerun()